{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in ./.pyenv/versions/3.6.3/lib/python3.6/site-packages\n",
      "Requirement already up-to-date: six in ./.pyenv/versions/3.6.3/lib/python3.6/site-packages (from nltk)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#for sentiment analysis (function: SentimentAnalysis())\n",
    "!{sys.executable} -m pip install -U nltk\n",
    "#sudo pip install -U nltk #OR #pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: requests in ./.pyenv/versions/3.6.3/lib/python3.6/site-packages\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in ./.pyenv/versions/3.6.3/lib/python3.6/site-packages (from requests)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in ./.pyenv/versions/3.6.3/lib/python3.6/site-packages (from requests)\n",
      "Requirement already up-to-date: urllib3<1.23,>=1.21.1 in ./.pyenv/versions/3.6.3/lib/python3.6/site-packages (from requests)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in ./.pyenv/versions/3.6.3/lib/python3.6/site-packages (from requests)\n",
      "Requirement already up-to-date: beautifulsoup4 in ./.pyenv/versions/3.6.3/lib/python3.6/site-packages\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#for crawling web data\n",
    "!{sys.executable} -m pip install -U requests\n",
    "#$ pipenv install requests\n",
    "\n",
    "!{sys.executable} -m pip install -U beautifulsoup4\n",
    "#$ pip3 install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrawlProductName(KickstarterURL):\n",
    "    ProductURL = KickstarterURL\n",
    "    return (ProductURL.rsplit('/',1)[1]) #capture only product name from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Before running code\n",
    "#1.make sure your python installed nltk, requests,beautifulsoup4 (type the following code inside terminal)\n",
    "#pip install -U nltk\n",
    "#pip install -U requests\n",
    "#pip install -U beautifulsoup4\n",
    "\n",
    "#2.make sure you have \"query_result.csv\" (which contains column \"uname\" and \"url\") inside directory. If no, please do the following\n",
    "#Go to Gdrive -->RMBI4980 --> Kickstarter --> new --> \"Kickstarter code.sql\"; run it inside MySQL/Sequel Pro\n",
    "#Type query: select id,uname,name,url from top100; run it and export as \"query_result.csv\"\n",
    "\n",
    "###Instruction\n",
    "#This coding is used to process sentiment analysis of all selected Kickstarter products:\n",
    "#-->Loop all URL of the product\n",
    "#--> Crawl comment data \n",
    "#--> Save in temp csv called \"ListOfComments.csv\"\n",
    "#--> Process sentiment analysis\n",
    "#--> Save all result in csv called \"CommentsAnalysis.csv\"\n",
    "\n",
    "#Remark: there is the function called randomSleep which is used to try avoiding to be blocked by the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/kawaiyim/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "###List of importing\n",
    "#For function \"make_soup\", \"crawl_comment\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#For function \"rawdata\", Comment_write_csv\", \"SentimentAnalysis\", \"Analysis_write_csv\", \"Analysis_append_csv\"\n",
    "import csv\n",
    "\n",
    "#For function \"SentimentAnalysis\"\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#For function \"rawdata\"\n",
    "from collections import defaultdict\n",
    "\n",
    "#For function \"randomSleep\"\n",
    "import time, random\n",
    "\n",
    "#For function \"RemoveTempCsv\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawdata():\n",
    "    columns = defaultdict(list) # each value in each column is appended to a list\n",
    "\n",
    "    with open('query_result.csv',encoding=\"latin-1\") as f:\n",
    "        reader = csv.DictReader(f) # read rows into a dictionary format\n",
    "        for row in reader: # read a row as {column1: value1, column2: value2,...}\n",
    "            for (k,v) in row.items(): # go over each column name and value \n",
    "                columns[k].append(v) # append the value into the appropriate list\n",
    "                                     # based on column name k\n",
    "\n",
    "    return(columns['uname'],columns['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    return BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSleep():\n",
    "        sleeptime =  random.randint(1, 2)\n",
    "        time.sleep(sleeptime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveTempCsv():\n",
    "    if os.path.exists('ListOfComments.csv'):\n",
    "        os.remove('ListOfComments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Comment_open_csv():\n",
    "    with open(\"ListOfComments.csv\",\"w\") as csv_file:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Comment_write_csv(data):\n",
    "    with open(\"ListOfComments.csv\",\"w\") as csv_file:\n",
    "        writer = csv.writer(csv_file,delimiter=',')\n",
    "        for comment in data:\n",
    "            writer.writerow([comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_comment(KickstarterURL):  #Input: product's page in Kickstarter Web\n",
    "    try:\n",
    "        ###Collect creator bio page of the product\n",
    "        soup=make_soup(KickstarterURL+'/creator_bio')\n",
    "        ###Try avoiding to be blocked, with a random short wait\n",
    "        randomSleep()\n",
    "        \n",
    "        ###For testing\n",
    "        #print(soup) \n",
    "        #print(soup.status_code) \n",
    "        #print(soup.prettify()) #show all coding in the web page\n",
    "        \n",
    "        ###Used later to check if the comment is made by creator\n",
    "        MainDesignerName=\"\"\n",
    "        RangeOfDesignerDetail = soup.find('div',class_=\"creator-bio-details col col-4 pt3 pb3 pb10-sm\")\n",
    "        MDN = RangeOfDesignerDetail.find('span',class_=\"identity_name\")\n",
    "        if MDN:\n",
    "            MainDesignerName = MDN.get_text()\n",
    "            MainDesignerName = MainDesignerName.replace('\\n','')\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        ###Collect comment page of the product\n",
    "        soup=make_soup(KickstarterURL+'/comments')\n",
    "        \n",
    "        ###Try avoiding to be blocked, with a random short wait\n",
    "        randomSleep()\n",
    "        \n",
    "        ###For testing\n",
    "        #print(soup) \n",
    "        #print(soup.status_code) \n",
    "        #print(soup.prettify()) #show all coding in the web page\n",
    "        \n",
    "        ###Find the area of comment in the webpage\n",
    "        RangeOfComment = soup.find('ol',class_=\"comments\")\n",
    "        #print(RangeOfComment) #show all coding in the whole area of comments if necessary\n",
    "        \n",
    "        ListOfCommenters=RangeOfComment.findAll('div',class_=\"main clearfix pl3 ml3\")\n",
    "        \n",
    "        ### Pull all text from the comments\n",
    "        x = 1\n",
    "        NoOfCommentFromCreator = 0\n",
    "        \n",
    "        data=[] #List of comments ready to be put into csv file\n",
    "        \n",
    "        # Get each comment by looping all commenters\n",
    "        for commenter in range(len(ListOfCommenters)):\n",
    "            CommenterName = ListOfCommenters[commenter].find('a',class_=\"author green-dark\").get_text()\n",
    "            if CommenterName != MainDesignerName: #if commenter is backer, then do sentiment analysis\n",
    "                print (\"Comment\" + str(x) + \": \")\n",
    "                print()\n",
    "\n",
    "                #Print each sentence in one line & combine into a complete sentence & store into \"data\" variable\n",
    "                Comment = ListOfCommenters[commenter].findAll(\"p\")\n",
    "                NumberOfSentence = len(Comment)\n",
    "\n",
    "                WholeSentence = []\n",
    "\n",
    "                for sentence in range(NumberOfSentence): #print all comments\n",
    "\n",
    "                    print (\"\".join((Comment[sentence].get_text())), sep='', end='\\n') #show all comments\n",
    "                    WholeSentence.append((Comment[sentence].get_text()))\n",
    "\n",
    "                WholeSentence = ''.join(WholeSentence)\n",
    "\n",
    "                data.append(WholeSentence)\n",
    "                print()\n",
    "                x += 1\n",
    "            if CommenterName == MainDesignerName:\n",
    "                    NoOfCommentFromCreator += 1\n",
    "        ###For testing\n",
    "        #print(data) #List of all comments\n",
    "        \n",
    "        ###Write to CSV\n",
    "        Comment_write_csv(data)\n",
    "        \n",
    "        return(NoOfCommentFromCreator)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "#Tutorial: https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "\n",
    "crawl_comment(\"https://www.kickstarter.com/projects/1218200025/naked-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analysis_write_csv():\n",
    "    with open(\"CommentsAnalysis.csv\",\"w\") as csv_file:\n",
    "        writer = csv.writer(csv_file,delimiter=',')\n",
    "        writer.writerow([\"Product Name\",\"Total number of comments (backer only)\",\"Number of Negative Comment\",\"Number of Neutral Comment\",\"Number of Positive Comment\",\"% of Negative Comment\",\"% of Neutral Comment\",\"% of Positive Comment\",\"Overall Score\",\"Number of creator comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analysis_append_csv(data,NoOfCreatorComment):\n",
    "    with open(\"CommentsAnalysis.csv\",\"a\") as csv_file:\n",
    "        if data:\n",
    "            if NoOfCreatorComment:\n",
    "                data.append(NoOfCreatorComment)\n",
    "\n",
    "            else:\n",
    "                data.append(0)\n",
    "                \n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow(data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentimentAnalysis(ProductName,csvFile):\n",
    "    try:\n",
    "        #ListOfReview = []\n",
    "\n",
    "        ###Open csv file containing a column of comments\n",
    "        with open(csvFile,'r') as f:\n",
    "            #for sentence in f:\n",
    "               # ListOfReview.append(sentence)\n",
    "            reader=csv.reader(f)\n",
    "            ListOfReview = list(reader)\n",
    "            ListOfReview = [l[0] for l in ListOfReview]\n",
    "\n",
    "        ###For testing\n",
    "        #print(ListOfReview) #List containing all comments\n",
    "\n",
    "        #For sentence in ListOfReview:\n",
    "        #    print(sentence) #Show each comment\n",
    "\n",
    "        ###Sentiment analysis using NLTK\n",
    "\n",
    "        print()\n",
    "        print(\"------------------Sentiment Analysis------------------\")\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        ListOfRate = []\n",
    "        x=1\n",
    "        SumOfScore = 0\n",
    "        \n",
    "        #Looping all comments\n",
    "        for sentence in ListOfReview:\n",
    "            print(\"Comment\"+ str(x) + \": \" + sentence)\n",
    "            print()\n",
    "            \n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            \n",
    "            for k in ss:\n",
    "                print('{0}: {1} '.format(k, ss[k]), end='')\n",
    "                print()\n",
    "            print()\n",
    "            \n",
    "            #Used for calculating average score of the product\n",
    "            if ss['compound']: \n",
    "                SumOfScore += float(ss['compound']) \n",
    "            \n",
    "            #Summary\n",
    "            if ss['compound'] < 0:\n",
    "                    ListOfRate.append(\"neg\")\n",
    "            elif ss['compound'] == 0:\n",
    "                    ListOfRate.append(\"neu\")\n",
    "            else:\n",
    "                    ListOfRate.append(\"pos\")\n",
    "            \n",
    "            x+=1\n",
    "\n",
    "        NoOfComments = len(ListOfRate)\n",
    "    \n",
    "        data = [ProductName]\n",
    "        \n",
    "        if NoOfComments != 0:\n",
    "            NoOfNeg = ListOfRate.count(\"neg\")\n",
    "            NoOfNeu = ListOfRate.count(\"neu\")\n",
    "            NoOfPos = ListOfRate.count(\"pos\")\n",
    "            RatioNeg = ListOfRate.count(\"neg\")/len(ListOfRate)\n",
    "            RatioNeu = ListOfRate.count(\"neu\")/len(ListOfRate)\n",
    "            RatioPos = ListOfRate.count(\"pos\")/len(ListOfRate)\n",
    "\n",
    "            if SumOfScore != 0 and NoOfComments !=0:\n",
    "                OverallScore = SumOfScore/NoOfComments\n",
    "            else:\n",
    "                OverallScore = 0\n",
    "            \n",
    "            data.append(NoOfComments)\n",
    "            data.append(NoOfNeg)\n",
    "            data.append(NoOfNeu)\n",
    "            data.append(NoOfPos)\n",
    "            data.append('%.2f'%RatioNeg)\n",
    "            data.append('%.2f'%RatioNeu)\n",
    "            data.append('%.2f'%RatioPos)\n",
    "            data.append(OverallScore)\n",
    "\n",
    "        if NoOfComments == 0:\n",
    "            NoOfNeg = 0\n",
    "            NoOfNeu = 0\n",
    "            NoOfPos = 0\n",
    "            RatioNeg = 0\n",
    "            RatioNeu = 0\n",
    "            RatioPos = 0\n",
    "\n",
    "            if SumOfScore != 0 and NoOfComments !=0:\n",
    "                OverallScore = SumOfScore/NoOfComments\n",
    "            else:\n",
    "                OverallScore = 0\n",
    "\n",
    "            data.append(NoOfComments)\n",
    "            data.append(NoOfNeg)\n",
    "            data.append(NoOfNeu)\n",
    "            data.append(NoOfPos)\n",
    "            data.append('%.2f'%RatioNeg)\n",
    "            data.append('%.2f'%RatioNeu)\n",
    "            data.append('%.2f'%RatioPos)\n",
    "            data.append(OverallScore)\n",
    "            \n",
    "        print()\n",
    "        print(\"------------------Summary------------------\")\n",
    "        print('{:30}'.format(\"List of Rating:\"), ListOfRate)\n",
    "        print('{:30}'.format(\"Total number of comments:\"), len(ListOfRate))\n",
    "        print('{:30}'.format(\"Number of negative comments: \"), NoOfNeg, \"(\", '{:.1%}'.format(RatioNeg), \")\")\n",
    "        print('{:30}'.format(\"Number of neutral comments: \"), NoOfNeu, \"(\", '{:.1%}'.format(RatioNeu), \")\")\n",
    "        print('{:30}'.format(\"Number of positive comments: \"), NoOfPos, \"(\", '{:.1%}'.format(RatioPos), \")\")\n",
    "        print('{:30}'.format(\"Overall score of product:\"), OverallScore)\n",
    "        print()\n",
    "        \n",
    "        return data #for next step: appending the data into csv file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    ListOfProductName = rawdata()[0]\n",
    "    ListOfKickstarterURL = rawdata()[1]\n",
    "    \n",
    "    Analysis_write_csv() #create a csv file storing result of sentiment analysis\n",
    "    \n",
    "    x = 0\n",
    "    \n",
    "    for KickstarterURL in ListOfKickstarterURL:\n",
    "        \n",
    "        ProductName = ListOfProductName[x] #Take the corresponding product name\n",
    "\n",
    "        RemoveTempCsv() #Delete the temp csv if it exists (in case it uses back the existing temp file for further process)\n",
    "        Comment_open_csv() #Create a new temp csv \n",
    "        \n",
    "        NoOfCreatorComment = crawl_comment(KickstarterURL)\n",
    "        Analysis_append_csv(SentimentAnalysis(ProductName,'ListOfComments.csv'),NoOfCreatorComment) #append analysis result into csv file\n",
    "        \n",
    "        x +=1 #Loop all selected product\n",
    "        \n",
    "        #Steps of core function:\n",
    "        #Function: crawl_comment(KickstarterURL)\n",
    "        #1. Crawl comment from website\n",
    "        #2. Put them into \"ListOfComments.csv\" file: function \"Comment_write_csv(data)\"\n",
    "        #3. Return the number of comment from creator --> Stored in NofCreatorComment\n",
    "        \n",
    "        #Function: Analysis_append_csv\n",
    "        #1. Process sentiment analysis using previous csv file \"ListOfComments.csv\"\n",
    "        #2. Put analysis result into \"CommentsAnalysis.csv\" file\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
